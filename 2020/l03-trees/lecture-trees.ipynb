{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"img/logo_hse_black.jpg\"></center>\n",
    "\n",
    "<h1><center>Data Analysis</center></h1>\n",
    "<h3><center>Andrey Shestakov (<a href=\"mailto:avshestakov@hse.ru\">avshestakov@hse.ru</a>)</center></h3>\n",
    "<hr>\n",
    "<h2><center>Decision trees<sup><a href=\"#fn1\" id=\"ref1\">1</a></sup></center></h2>\n",
    "\n",
    "\n",
    "\n",
    "<sup id=\"fn1\">1. Some materials are taken from <a href=\"http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%82%D0%B5%D0%BC%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D1%8B_%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F_%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%92.%D0%92.%D0%9A%D0%B8%D1%82%D0%BE%D0%B2%29\">machine learning course of Victor Kitov</a></sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.rcParams['figure.figsize'] = (15,10)\n",
    "\n",
    "# Для кириллицы на графиках\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "try:\n",
    "    from ipywidgets import interact, IntSlider, fixed, FloatSlider\n",
    "except ImportError:\n",
    "    print(u'Так надо')\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's recall previous lecture\n",
    "\n",
    "* Metric methods: Nearest Centroid, K Nearest Neighbours\n",
    "    * Work both for classification and regression\n",
    "    * Lazy learning - simply remember training dataset\n",
    "    * No parameters - only hyper-parameters\n",
    "* Cluster hypothesis - the core of metric methods\n",
    "* Similarity measures and distances: euclidean, cosine, edit-distance, Jaccard similarity, etc...\n",
    "* Feature scaling is important!\n",
    "* Various modifications:\n",
    "    * weighted domain\n",
    "* Get ready to face with\n",
    "    * Curse of dimentionality (about that in the next lectures)\n",
    "    * Slow prediction speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intuition 1\n",
    "\n",
    "* A perfumery company developed a new unisex parfume\n",
    "* To find their key segments it they run open world testing\n",
    "* Each respondent leaves \n",
    "    * responce if she likes it or not (`+1`|`-1`)\n",
    "    * some info about her\n",
    "        * Gender\n",
    "        * Age\n",
    "        * Education\n",
    "        * Current career\n",
    "        * Have domestic animals\n",
    "        * etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intuition 1\n",
    "\n",
    "In the end the description of the segments could look like this\n",
    "\n",
    "* `[Gender = F][Age > 21][Age <= 25][Education = Higher][Have domestic animals = No]`  - like in 82% of cases\n",
    "* `[Gender = M][Age > 25][Age <= 30][Current Career = Manager]` - don't like in 75% of cases\n",
    "* ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intuition 2\n",
    "\n",
    "* You are going to take a loan ~~god, please, no~~ to buy something expensive, and provide your application form\n",
    "* Bank employee is checking it accoring to some rules like:\n",
    "    1. Current bank account > 200k rubles. - go to step 2, otherwise 3\n",
    "    2. Duration < 30 months - go to step 4, otherwise REJECT\n",
    "    3. Current employment > 1 year - ACCEPT, otherwise REJECT\n",
    "    4. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intuition 2\n",
    "<center><img src='https://mapr.com/blog/predicting-loan-credit-risk-using-apache-spark-machine-learning-random-forests/assets/blogimages/creditdecisiontree.png'><center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intuition 3\n",
    "\n",
    "<center><img src='https://static01.nyt.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg'><center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intuition 4\n",
    "<center><img src='https://i.stack.imgur.com/KYSy4.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intuition 4\n",
    "<center><img src='img/Decision_tree_model.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='https://eight2late.files.wordpress.com/2016/02/7214525854_733237dd83_z1.jpg?w=700'><center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Definition of decision tree\n",
    "\n",
    "* Prediction is performed by tree $T$ (directed, connected, acyclic graph)\n",
    "\n",
    "* Node types\n",
    "    1. A root node\n",
    "    2. Internal nodes, each having $\\ge2$ child nodes\n",
    "    3. Terminal nodes (leaves), which do not have child nodes but have associated prediction values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Definition of decision tree\n",
    "\n",
    "* for each non-terminal node $t$ a **check-function** $Q_{t}(x)$ is associated \n",
    "* for each edge $r_{t}(1),...r_{t}(K_{t})$ a set of values of check-function\n",
    "$Q_{t}(x)$ is associated: $S_{t}(1),...S_{t}(K_{t})$ such that:\n",
    "    * $\\bigcup_{k}S_{t}(k)=range[Q_{t}]$ \n",
    "    * $S_{t}(i)\\cap S_{t}(j)=\\emptyset$ $\\forall i\\ne j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prediction process\n",
    "\n",
    "* Prediction is easy if we have already constructed a tree\n",
    "* Prediction process for tree $T$:\n",
    "    * $t=root(T)$\n",
    "    * while $t$ is not a terminal node:\n",
    "\n",
    "        * calculate $Q_{t}(x)$\n",
    "        * determine $j$ such that $Q_{t}(x)\\in S_{t}(j)$\n",
    "        * follow edge $r_{t}(j)$ to $j$-th child node: $t=\\tilde{t}_{j}$\n",
    "    * return prediction, associated with leaf $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Specification of decision tree\n",
    "* To define a decision tree one needs to specify:\n",
    "    * the check-function: $Q_{t}(x)$ \n",
    "    * the splitting criterion: $K_{t}$ and $S_{t}(1),...S_{t}(K_{t})$\n",
    "    * the termination criteria (when node is defined as a terminal node)\n",
    "    * the predicted value for each leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generalized decision tree algorithm\n",
    "\n",
    "```{python}\n",
    "1. function decision_tree(X, y):\n",
    "\n",
    "2.    if termination_criterion(X, y) == True:\n",
    "    \n",
    "3.        S = create_leaf_with_prediction(y)\n",
    "        \n",
    "4.    else:\n",
    "    \n",
    "5.        S = create_node()\n",
    "6.        (X_1, y_1) .. (X_L, y_L) = best_split(X, y)\n",
    "        \n",
    "7.        for i in 1..L:\n",
    "8.            C = decision_tree(X_i, y_i)\n",
    "9.            connect_nodes(S, C)\n",
    "10.   return S     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Splitting rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Possible definitions of splitting rules\n",
    "\n",
    "* $Q_{t}(x)=x^{i(t)}$, where $S_{t}(j)=v_{j}$, where $v_{1},...v_{K}$ are unique values of feature $x^{i(t)}$.\n",
    "* $S_{t}(1)=\\{x^{i(t)}\\le h_{t}\\},\\,S_{t}(2)=\\{x^{i(t)}>h_{t}\\}$\n",
    "* $S_{t}(j)=\\{h_{j}<x^{i(t)}\\le h_{j+1}\\}$ for set of partitioning thresholds $h_{1},h_{2},...h_{K_{t}+1}$.\n",
    "* $S_{t}(1)=\\{x:\\,\\langle x,v\\rangle\\le0\\},\\quad S_{t}(2)=\\{x:\\,\\langle x,v\\rangle>0\\}$\n",
    "* $S_{t}(1)=\\{x:\\,\\left\\lVert x\\right\\rVert \\le h\\},\\quad S_{t}(2)=\\{x:\\,\\left\\lVert x\\right\\rVert >h\\}$\n",
    "* etc.\n",
    "<center><img src='img/Decision_tree_model.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Most famous decision tree algorithms\n",
    "\n",
    "* C4.5\n",
    "* ID 3\n",
    "* CART (classification and regression trees)\n",
    "    * implemented in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CART version of splitting rule\n",
    "\n",
    "* single feature value is considered:\n",
    "$$\n",
    "Q_{t}(x)=x^{i(t)}\n",
    "$$\n",
    "* binary splits:\n",
    "$$\n",
    "K_{t}=2\n",
    "$$\n",
    "* split based on threshold $h_{t}$:\n",
    "$$\n",
    "S_{1}=\\{x^{i(t)}\\le h_{t}\\},\\,S_{2}=\\{x^{i(t)}>h_{t}\\}\n",
    "$$\n",
    "* $h(t)\\in\\{x_{1}^{i(t)},x_{2}^{i(t)},...x_{N}^{i(t)}\\}$\n",
    "\n",
    "    * applicable only for real, ordinal and binary features\n",
    "    * what about categorical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Splitting rule selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intuition\n",
    "\n",
    "Which box is better to predict color\n",
    "\n",
    "<center><img src='img/bins.png'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification impurity functions\n",
    "\n",
    "* For classification: let $p_{1},...p_{C}$ be class probabilities for objects in node $t$.\n",
    "* Then impurity function $\\phi(t)=\\phi(p_{1},p_{2},...p_{C})$ should satisfy:\n",
    "\n",
    "    * $\\phi$ is defined for $p_{j}\\ge0$ and $\\sum_{j}p_{j}=1$.\n",
    "    * $\\phi$ attains maximum for $p_{j}=1/C,\\,k=1,2,...C$ .\n",
    "    * $\\phi$ attains minimum when $\\exists j:\\,p_{j}=1,\\,p_{i}=0$ $\\forall i\\ne j$.\n",
    "    * $\\phi$ is symmetric function of $p_{1},p_{2},...p_{C}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Typical classification impurity functions}\n",
    "\n",
    "* **Gini criterion**\n",
    "\n",
    "    * interpretation: probability to make mistake when predicting class randomly with class probabilities $[p(\\omega_{1}|t),...p(\\omega_{C}|t)]$:\n",
    "$$\n",
    "I(t)=\\sum_{i}p(\\omega_{i}|t)(1-p(\\omega_{i}|t))=1-\\sum_{i}[p(\\omega_{i}|t)]^{2}\n",
    "$$\n",
    "\n",
    "* **Entropy**\n",
    "\n",
    "    * interpretation: measure of uncertainty of random variable\n",
    "$$\n",
    "I(t)=-\\sum_{i}p(\\omega_{i}|t)\\ln p(\\omega_{i}|t)\n",
    "$$\n",
    "\n",
    "* **Classification error**\n",
    "     * interpretation: frequency of errors when classifying with the most common class\n",
    "$$\n",
    "I(t)=1-\\max_{i}p(\\omega_{i}|t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def plot_impurities():\n",
    "    p = np.linspace(0, 1, 100)\n",
    "    p = np.c_[p, 1-p]\n",
    "\n",
    "    missclass = 1 - p.max(axis=1)\n",
    "    plt.plot(p[:,0], missclass, label = 'missclassification error')\n",
    "\n",
    "    gini = 1 - (p ** 2).sum(axis=1)\n",
    "    plt.plot(p[:,0], gini, label = 'gini index')\n",
    "\n",
    "    entropy = - np.nansum((p*np.log2(p)), axis=1)\n",
    "    plt.plot(p[:,0], entropy, label = 'entropy')\n",
    "\n",
    "    plt.xlabel('$p(\\omega_1|t)$')\n",
    "    plt.ylabel('$I(t)$')\n",
    "    # plt.legend(loc=2, bbox_to_anchor=(0.,0.))\n",
    "    plt.legend(loc=2, bbox_to_anchor=(-0.3,1))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_impurities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Splitting criterion selection\n",
    "\n",
    "* Define $\\Delta I(t)$ - the quality of the split of node $t$ into child nodes $t_{1},...t_{C}$.\n",
    "$$\n",
    "\\Delta I(t)=I(t)-\\sum_{i=1}^{C}I(t_{i})\\frac{N(t_{i})}{N(t)}\n",
    "$$\n",
    "$$\n",
    "\\Delta I(t)=I(t)-\\left(I(t_{L})\\frac{N(t_{L})}{N(t)} + I(t_{R})\\frac{N(t_{R})}{N(t)}\\right)\n",
    "$$\n",
    "\n",
    "    * If $I(t)$ is entropy, then $\\Delta I(t)$ is called *information gain*.\n",
    "\n",
    "\n",
    "* CART optimization (regression, classification): select feature $i_{t}$ and threshold $h_{t}$, which maximize $\\Delta I(t)$:\n",
    "$$\n",
    "i_{t},\\,h_{t}=\\arg\\max_{k,h}\\Delta I(t)\n",
    "$$\n",
    "* CART decision making: from node $t$ follow: \n",
    "\n",
    "$$\\begin{cases}\n",
    "\\text{left child }t_{1}, & \\text{if }x^{i_{t}}\\le h_{t}\\\\\n",
    "\\text{right child }t_{2}, & \\text{if }x^{i_{t}}>h_{t}\n",
    "\\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def impurity(p): \n",
    "    return -np.sum(p*np.log(p))\n",
    "\n",
    "def wine_demo():\n",
    "\n",
    "    df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "    fig.set_figheight(5)\n",
    "\n",
    "    df_wine.loc[:, 'quality_cat'] = (df_wine.loc[:, 'quality'] > 5).astype(int) \n",
    "    idx = df_wine.loc[:, 'quality_cat'] == 1\n",
    "    df_wine.loc[idx, 'alcohol'].hist(label='good quality', bins=20, alpha = 0.4, ax=ax[0])\n",
    "    df_wine.loc[~idx, 'alcohol'].hist(label='bad quality', bins=20, alpha = 0.4, ax=ax[0])\n",
    "    ax[0].set_xlabel('alcohol')\n",
    "\n",
    "    p = np.array([df_wine.quality_cat.mean(), 1-df_wine.quality_cat.mean()])\n",
    "\n",
    "    init_impurity = impurity(p)\n",
    "\n",
    "    G = []\n",
    "    t_range = np.linspace(df_wine.alcohol.min(), df_wine.alcohol.max(), 100)\n",
    "\n",
    "    for t in t_range:\n",
    "        idx = df_wine.alcohol < t\n",
    "        p1 = np.array([df_wine.loc[idx, 'quality_cat'].mean(), 1-df_wine.loc[idx, 'quality_cat'].mean()])\n",
    "        p2 = np.array([df_wine.loc[~idx, 'quality_cat'].mean(), 1-df_wine.loc[~idx, 'quality_cat'].mean()])\n",
    "\n",
    "        G.append(init_impurity - (idx.mean()*impurity(p1) + (1-idx.mean())*impurity(p2)))\n",
    "\n",
    "    ax[1].plot(t_range, G)\n",
    "    ax[1].set_xlabel('alcohol')\n",
    "    ax[1].set_ylabel('Gain')\n",
    "\n",
    "    mG = np.nanmax(G)\n",
    "    mt = t_range[np.nanargmax(G)]\n",
    "\n",
    "    ax[0].vlines(mt, 0, 150, label='best threshold (%.2f)' % mt)\n",
    "    ax[1].vlines(mt, 0, mG, label='best threshold\\n(gain = %.4f)' % mG)\n",
    "    \n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "wine_demo() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Typical regression impurity functions\n",
    "\n",
    "* Impurity function measures uncertainty in $y$ for objects falling inside node $t$.\n",
    "    * Regression:\n",
    "        * let objects falling inside node $t$ be $I=\\{i_{1},...i_{K}\\}$. We may define\n",
    "\\begin{align*}\n",
    "\\phi(t) & =\\frac{1}{K}\\sum_{i\\in I}\\left(y_{i}-\\mu\\right)^{2}\\quad \\text{(MSE)}\\\\\n",
    "\\phi(t) & =\\frac{1}{K}\\sum_{i\\in I}|y_{i}-\\mu|\\quad \\text{(MAE)}\n",
    "\\end{align*}\n",
    "where $\\mu$ is `mean` or  `median` of $y_i$s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prediction assignment to leaves\n",
    "\n",
    "* Regression:\n",
    "    * mean (optimal for MSE loss)\n",
    "    * median (optimal for MAE loss)\n",
    "\n",
    "\n",
    "* Classification\n",
    "    * most common class (optimal for constant misclassification cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import plot_tree\n",
    "from ipywidgets import interact, IntSlider\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def demo_dec_tree(depth=1):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20, 10))\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    C = np.array([[0., -0.7], [1.5, 0.7]])\n",
    "    gauss1 = np.dot(np.random.randn(200, 2) + np.array([4, 2]), C)\n",
    "    gauss2 = np.dot(np.random.randn(300, 2), C)\n",
    "\n",
    "    X = np.vstack([gauss1, gauss2])\n",
    "    y = np.r_[np.ones(200), np.zeros(300)]\n",
    "\n",
    "    ax[1].set_xlabel('$x_1$')\n",
    "    ax[1].set_ylabel('$x_2$')\n",
    "\n",
    "    # Dec Tree Stuff\n",
    "    tree = DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=123)\n",
    "    tree.fit(X,y)\n",
    "\n",
    "    x_range = np.linspace(X.min(), X.max(), 100)\n",
    "    xx1, xx2 = np.meshgrid(x_range, x_range)\n",
    "\n",
    "    Y = tree.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "    Y = Y.reshape(xx1.shape)\n",
    "\n",
    "    ax[1].contourf(xx1, xx2, Y, alpha=0.3, cmap=plt.cm.Paired)\n",
    "    ax[1].scatter(X[:,0], X[:,1],c=y, cmap=plt.cm.Paired)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        plot_tree(tree, ax=ax[0], filled=True, feature_names=['$x_1$', '$x_2$'])\n",
    "        ax[0].axis(\"off\")\n",
    "    except:\n",
    "        print('Скорее всего не установлен graphviz')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig = interact(demo_dec_tree, depth=IntSlider(min=1, max=5, value=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def plot_dec_reg(depth=1, criterion='mse', ):\n",
    "        \n",
    "    np.random.seed(123)\n",
    "    x_true = np.arange(-5, 5, 0.2)\n",
    "    x = x_true + np.random.rand(x_true.shape[0]) - 0.5\n",
    "    y_true = np.sin(x_true)+x_true/3\n",
    "    y = y_true + np.random.rand(x_true.shape[0]) - 0.5\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "    \n",
    "    tree = DecisionTreeRegressor(criterion=criterion, max_depth=depth)\n",
    "    tree.fit(x.reshape(-1,1), y)\n",
    "    x_pred = np.arange(-5, 5, 0.1)\n",
    "    y_hat = tree.predict(x_pred.reshape(-1,1))\n",
    "    \n",
    "    ax[1].plot(x_true, y_true, c='g', label='$f(x)$')\n",
    "    ax[1].scatter(x, y, label='actual data', cmap=plt.cm.Paired)\n",
    "    ax[1].set_xlabel('x')\n",
    "    ax[1].set_ylabel('y')\n",
    "    ax[1].plot(x_pred, y_hat, c='r', label='decision tree \\nregression')\n",
    "    ax[1].legend(loc=2)\n",
    "    \n",
    "    try:\n",
    "        plot_tree(tree, ax=ax[0], filled=True, feature_names=['$x$'])\n",
    "        ax[0].axis(\"off\")\n",
    "    except:\n",
    "        print('Скорее всего не установлен graphviz')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = interact(plot_dec_reg, depth=IntSlider(min=1, max=5, value=1), criterion=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Splitting criterion selection\n",
    "\n",
    "**Remarks**\n",
    "* Local and Greedy optimization\n",
    "* Overall results changes slighly with different impurity measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n",
    "                     np.linspace(-3, 3, 50))\n",
    "rng = np.random.RandomState(0)\n",
    "X_ = rng.randn(200, 2)\n",
    "y_ = np.logical_xor(X_[:, 0] > 0, X_[:, 1] > 0)\n",
    "idx = np.random.choice(range(200), 10)\n",
    "y_[idx] = ~y_[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_[:, 0], X_[:, 1], c=y_, cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def demo_dec_tree_xor(depth=1):\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n",
    "                     np.linspace(-3, 3, 50))\n",
    "    rng = np.random.RandomState(0)\n",
    "    X = rng.randn(200, 2)\n",
    "    y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\n",
    "    idx = np.random.choice(range(200), 10)\n",
    "    y[idx] = ~y[idx]\n",
    "\n",
    "\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "\n",
    "    # Dec Tree Stuff\n",
    "    tree = DecisionTreeClassifier(criterion='entropy', max_depth=depth, random_state=123)\n",
    "    tree.fit(X,y)\n",
    "\n",
    "    x_range = np.linspace(X.min(), X.max(), 100)\n",
    "    xx1, xx2 = np.meshgrid(x_range, x_range)\n",
    "\n",
    "    Y = tree.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "    Y = Y.reshape(xx1.shape)\n",
    "\n",
    "    ax.contourf(xx1, xx2, Y, alpha=0.3, cmap=plt.cm.Paired,\n",
    "            edgecolors=(0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = interact(demo_dec_tree_xor, depth=IntSlider(min=1, max=6, value=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Termination criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Termination criterion\n",
    "\n",
    "* Tradeoff:\n",
    "    * very large complex trees -> overfitting\n",
    "    * very short simple trees -> underfitting\n",
    "\n",
    "\n",
    "* Approaches to stop DC construction:\n",
    "    * rule-based stopping criterion\n",
    "    * based on pruning (not considered here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Rule-based termination criteria\n",
    "\n",
    "* Rule-based: a criterion is compared with a threshold.\n",
    "* Variants of criterion:\n",
    "    * depth of tree\n",
    "    * number of objects in a node\n",
    "    * minimal number of objects in one of the child nodes\n",
    "    * impurity of classes\n",
    "    * change of impurity of classes after the split\n",
    "    * etc\n",
    "\n",
    "#### Advantages:\n",
    "* simplicity\n",
    "* interpretability\n",
    "\n",
    "#### Disadvantages:\n",
    "* specification of threshold is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CART Cost-Complexity Prunning\n",
    "\n",
    "* General idea: build tree up to pure nodes and then prune.\n",
    "* Define:\n",
    "    * $T$ be some subtree of our tree\n",
    "    * $T_t$ full subtree with root at node $t$\n",
    "    * $\\tilde{T}$ be a set of leaf nodes of tree $T$\n",
    "    * $R(t)$ - error measure inside node $t$ (#misclassifications, sum of squared errors)\n",
    "\n",
    "Error rate on tree:\n",
    "$$R(T) = \\sum\\limits_{\\tau \\in \\tilde{T}} R(\\tau)$$\n",
    "\n",
    "Error rate + complexity:\n",
    "$$R_\\alpha(T) = \\sum\\limits_{\\tau \\in \\tilde{T}} R(\\tau)+ \\alpha |T|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Generally $R(T_t) < R(t)$, however if we consider $R_\\alpha(\\cdot)$...\n",
    "* We can find $\\alpha$ such that $R_\\alpha(T_t) = R_\\alpha(t)$\n",
    "$$ \\alpha_t = \\frac{R(t) - R(T_t)}{|\\tilde{T_t}| - 1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The algorithm\n",
    "\n",
    "1. Build the most \"puriest\" tree $T_0$ that and set $\\alpha_0 = 0$, $i=0$\n",
    "2. Until the tree is completely prunned do:\n",
    "    * `i++`    \n",
    "    * find node $t$ that minimizes\n",
    "    $$ \\alpha_i = \\frac{R(t) - R(T_t)}{|\\tilde{T_t}| - 1} $$\n",
    "    * Replace $T_t$ with $t$\n",
    "    \n",
    "Output:\n",
    "* sequence of $\\alpha_0 \\leq \\alpha_1 \\leq  \\dots \\leq \\alpha_K$\n",
    "* with correspondent prunned tries $T_0 \\supseteq T_1 \\supseteq \\dots \\supseteq T_K$\n",
    "* choose $T_i$ with lowest error on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def plot_dec_reg_alpha(alpha=0):\n",
    "        \n",
    "    np.random.seed(123)\n",
    "    x_true = np.arange(-5, 5, 0.2)\n",
    "    x = x_true + np.random.rand(x_true.shape[0]) - 0.5\n",
    "    y_true = np.sin(x_true)+x_true/3\n",
    "    y = y_true + np.random.rand(x_true.shape[0]) - 0.5\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "    \n",
    "    tree = DecisionTreeRegressor(max_depth=None, ccp_alpha=alpha)\n",
    "    tree.fit(x.reshape(-1,1), y)\n",
    "    x_pred = np.arange(-5, 5, 0.1)\n",
    "    y_hat = tree.predict(x_pred.reshape(-1,1))\n",
    "    \n",
    "    ax[1].plot(x_true, y_true, c='g', label='$f(x)$')\n",
    "    ax[1].scatter(x, y, label='actual data', cmap=plt.cm.Paired)\n",
    "    ax[1].set_xlabel('x')\n",
    "    ax[1].set_ylabel('y')\n",
    "    ax[1].plot(x_pred, y_hat, c='r', label='decision tree \\nregression')\n",
    "    ax[1].legend(loc=2)\n",
    "    \n",
    "    try:\n",
    "        plot_tree(tree, ax=ax[0], filled=True, feature_names=['$x$'])\n",
    "        ax[0].axis(\"off\")\n",
    "    except:\n",
    "        print('Скорее всего не установлен graphviz')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = interact(plot_dec_reg_alpha, alpha=FloatSlider(min=0, max=0.05, value=0, step=0.0005, readout_format='.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tree feature importances\n",
    "\n",
    "* Consider feature $f$\n",
    "* Let $T(f)$ be the set of all nodes, relying on feature $f$ when making split.\n",
    "    * efficiency of split at node $t$: $\\Delta I(t)=I(t)-\\sum_{c\\in childen(t)}\\frac{n_{c}}{n_{t}}I(c)$\n",
    "    * feature importance of $f$: $\\sum_{t\\in T(f)}n_{t}\\Delta I(t)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Handling missing values\n",
    "\n",
    "1. Remove features or objects with missing values\n",
    "2. Missing value = distinct feature value\n",
    "3. Calculation of impurity w/o missing cases\n",
    "4. **Surrogate split!**\n",
    "    * Find best split with feature $i^*$, threshold $h^*$ and children $\\{t^*_L, t^*_R\\}$\n",
    "    * Find other good splits for features $i_t \\neq i^*$, s.t. $\\{t_L, t_R\\} \\approx \\{t^*_L, t^*_R\\}$\n",
    "    * While performing prediction of object $x$:\n",
    "        * If $x^{i^*}$ is `Null`, try $x^{i_t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Analysis of decision trees\n",
    "\n",
    "* **Advantages:**\n",
    "\n",
    "    * simplicity of algorithm\n",
    "    * interpretability of model (for short trees)\n",
    "    * implicit feature selection\n",
    "    * good for features of different nature:\n",
    "        * naturally handles both discrete and real features\n",
    "        * prediction is invariant to monotone transformations of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Analysis of decision trees\n",
    "\n",
    "\n",
    "* **Disadvantages:**\n",
    "    * not very high accuracy:\n",
    "        * high overfitting of tree structure\n",
    "        * non-parallel to axes class separating boundary may lead to many nodes in the tree for $Q_{t}(x)=x^{i(t)}$\n",
    "        * one step ahead lookup strategy for split selection may be insufficient (XOR example)\n",
    "    * not online - slight modification of the training set will require full tree reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Special Desicion Tree Algorithms\n",
    " \n",
    " \n",
    "**ID 3**\n",
    "* Categorical features only\n",
    "* Number of children = number of categories\n",
    "* Maximum depth\n",
    "\n",
    "**С 4.5**\n",
    "* Handling continious features\n",
    "* And categorical as in ID3\n",
    "* Find missing value - proceed down to all paths and average\n",
    "* Some prunning procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Interpretation of DT\n",
    "<center><img src='img/trees-interpr.png' width='750'></center>\n",
    "\n",
    "* http://blog.datadive.net/interpreting-random-forests/\n",
    "* https://eli5.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## References\n",
    "* [How tree works](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "* Mohammed J. Zaki, et al: Data Mining and Analysis - Fundamental Concepts and Algorithms - Chapter 19\n",
    "* Andrew R. Webb, et al: Statistical Pattern Recognition - Chapter 7\n",
    "* L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984.\n",
    "* Cost-complexity prunning in [sklearn](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py) + [example](http://mlwiki.org/index.php/Cost-Complexity_Pruning)\n"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/1f8c4751e12938961e423759861e6e5a"
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "gist": {
   "data": {
    "description": "CloudMail/hse-da-course/raw/lecture-intro/lecture-intro-v01.ipynb",
    "public": false
   },
   "id": "1f8c4751e12938961e423759861e6e5a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "livereveal": {
   "theme": "serif",
   "transition": "concave"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "513px",
    "width": "253px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "32px",
   "left": "9px",
   "right": "1379px",
   "top": "33px",
   "width": "212px"
  },
  "widgets": {
   "state": {
    "54e80d57f79b4bfc934a2b84cf5fe7ba": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    },
    "5fb17a3592634a4fba98446dacd6db43": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "6f6f6ce7b81743308b92966f225862a8": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
